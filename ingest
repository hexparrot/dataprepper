#!/usr/bin/env python3
import re
import os
import json
import pymongo
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# MongoDB Connection
MONGO_USER = os.getenv("MONGO_USER", "admin")
MONGO_PASSWORD = os.getenv("MONGO_PASSWORD", "password")
MONGO_HOST = "localhost"
MONGO_PORT = "27017"
MONGO_URI = f"mongodb://{MONGO_USER}:{MONGO_PASSWORD}@{MONGO_HOST}:{MONGO_PORT}/admin"

# Define base directory where all source data is stored
BASE_DATA_DIR = "userdata/purposed"

# Load data source mapping
DATA_SOURCES_FILE = "data_sources.json"
try:
    with open(DATA_SOURCES_FILE, "r") as f:
        data_sources = json.load(f)
except Exception as e:
    logging.error(f"Error loading {DATA_SOURCES_FILE}: {e}")
    data_sources = {}


def get_mongo_client():
    """Establishes and returns a MongoDB client."""
    return pymongo.MongoClient(MONGO_URI)


def sanitize_field_name(field_name):
    """Sanitizes field names by replacing spaces and special characters with underscores."""
    return re.sub(r"[^a-zA-Z0-9_]", "_", field_name)


def convert_timestamp(doc):
    """Converts 'timestamp' field to datetime if present."""
    if isinstance(doc, dict):
        for key in doc.keys():
            if "timestamp" in key.lower():  # Catch variations like 'Timestamp'
                try:
                    doc[key] = datetime.fromisoformat(doc[key].replace("Z", "+00:00"))
                except (ValueError, TypeError, AttributeError):
                    logging.debug(f"No conversion possible for {key}: {doc[key]}")
                    # this can happen for canceled rides
    return doc


def document_exists(collection, doc):
    """Checks if a document already exists in the collection."""
    return collection.count_documents(doc, limit=1) > 0


def ingest_json_files():
    """Iterates through JSON files in all directories and ingests them into the correct MongoDB database."""
    client = get_mongo_client()

    if not os.path.exists(BASE_DATA_DIR):
        logging.warning(f"Base data directory '{BASE_DATA_DIR}' does not exist.")
        return

    # Loop through all databases in data_sources.json
    for db_name, collections in data_sources.items():
        db_path = os.path.join(BASE_DATA_DIR, db_name)
        if not os.path.exists(db_path):
            logging.warning(f"Skipping missing directory: {db_path}")
            continue

        logging.info(f"Processing database: {db_name}")
        db = client[db_name]

        # Loop through JSON files in this database's folder
        for filename in os.listdir(db_path):
            if filename.endswith(".json"):
                collection_name = os.path.splitext(filename)[0]
                collection = db[collection_name]
                file_path = os.path.join(db_path, filename)

                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        if not data:
                            logging.info(f"Skipping empty JSON file: {filename}")
                            continue

                        # Apply sanitization and timestamp conversion
                        data = (
                            [
                                convert_timestamp(
                                    {sanitize_field_name(k): v for k, v in doc.items()}
                                )
                                for doc in data
                            ]
                            if isinstance(data, list)
                            else [
                                {
                                    sanitize_field_name(k): v
                                    for k, v in convert_timestamp(data).items()
                                }
                            ]
                        )

                        # Insert data, avoiding duplicates
                        if collection.estimated_document_count() == 0:
                            collection.insert_many(data)
                            logging.info(
                                f"Inserted {len(data)} documents into new collection '{collection_name}'."
                            )
                        else:
                            duplicate_count = 0
                            for doc in data:
                                if not document_exists(collection, doc):
                                    collection.insert_one(doc)
                                else:
                                    duplicate_count += 1
                            logging.info(
                                f"Skipped {duplicate_count} duplicate documents in '{collection_name}'."
                            )

                except Exception as e:
                    logging.error(f"Error processing {filename} in {db_name}: {e}")

    client.close()


if __name__ == "__main__":
    ingest_json_files()
