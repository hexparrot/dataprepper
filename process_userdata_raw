#!/usr/bin/env python3

import os
import sys
import re
import json
import subprocess
import hashlib
import exifread
from datetime import datetime
import xml.etree.ElementTree as ET

# Attempt to import your Reddit parser classes (if available)
try:
    from xform.reddit_parser import (
        RedditStatisticsParser,
        RedditCommentHeadersParser,
        RedditPostsHeadersParser,
        RedditCommentsParser,
        RedditPostsParser,
    )

    REDDIT_AVAILABLE = True
except ImportError:
    print(
        "[WARNING] Could not import xform.reddit_parser. Reddit parsing will be skipped."
    )
    REDDIT_AVAILABLE = False


# ------------------------------------------------------------------------------
# 1. Define Directory Hierarchy
# ------------------------------------------------------------------------------
USERDATA_DIR = "userdata"  # Project root
RAW_DIR = os.path.join(USERDATA_DIR, "raw")
TRANSFORMED_DIR = os.path.join(USERDATA_DIR, "transformed")
PURPOSEBUILT_DIR = os.path.join(USERDATA_DIR, "purposebuilt")

# Within `raw/`:
RAW_IMAGES = os.path.join(RAW_DIR, "images")
RAW_CHAT = os.path.join(RAW_DIR, "chat")
RAW_MYANIMELIST = os.path.join(RAW_DIR, "myanimelist")
RAW_REDDIT = os.path.join(RAW_DIR, "reddit")
RAW_YOUTUBE = os.path.join(RAW_DIR, "youtube")

# Within `transformed/`:
X_IMAGES = os.path.join(TRANSFORMED_DIR, "images")
X_CHAT = os.path.join(TRANSFORMED_DIR, "chat")
X_MYANIMELIST = os.path.join(TRANSFORMED_DIR, "myanimelist")
X_REDDIT = os.path.join(TRANSFORMED_DIR, "reddit")
X_YOUTUBE = os.path.join(TRANSFORMED_DIR, "youtube")

# If you need purpose-built outputs, they'd go in PURPOSEBUILT_DIR:
# e.g., PB_TIMELINE = os.path.join(PURPOSEBUILT_DIR, "timeline")


# ------------------------------------------------------------------------------
# 2. Helpers (Checksum, GPS conversion, date extraction)
# ------------------------------------------------------------------------------
def calculate_checksum(file_path):
    """Compute SHA-256 checksum for unique mapping."""
    hash_func = hashlib.sha256()
    with open(file_path, "rb") as f:
        while chunk := f.read(8192):
            hash_func.update(chunk)
    return hash_func.hexdigest()


def convert_gps_to_decimal(gps_values):
    """Convert GPS coordinates from [degrees, minutes, seconds/fraction] to decimal format."""
    try:
        degrees = float(gps_values[0])
        minutes = float(gps_values[1]) / 60
        if hasattr(gps_values[2], "num"):
            # seconds fraction
            seconds = float(gps_values[2].num) / float(gps_values[2].den) / 3600
        else:
            seconds = float(gps_values[2]) / 3600
        return degrees + minutes + seconds
    except Exception:
        return None


def extract_date_from_filename(filename):
    """
    Return 'YYYY-MM-DD' if present in filename; otherwise '1970-01-01'.
    Example: '2004-02-20 [Friday].htm' -> '2004-02-20'
    """
    match = re.search(r"\d{4}-\d{2}-\d{2}", filename)
    if match:
        return match.group(0)
    return "1970-01-01"


# ------------------------------------------------------------------------------
# 3. Image EXIF Extraction
# ------------------------------------------------------------------------------
def extract_exif(file_path):
    """Extract key EXIF metadata from an image."""
    with open(file_path, "rb") as img_file:
        tags = exifread.process_file(img_file, details=True)

    if not tags:
        print(f"[WARNING] No EXIF data found for {file_path}")
        return None

    metadata = {}
    metadata["author"] = "unspecified"
    metadata["FilePath"] = file_path
    metadata["Checksum"] = calculate_checksum(file_path)
    metadata["Make"] = str(tags.get("Image Make", "Unknown"))
    metadata["Model"] = str(tags.get("Image Model", "Unknown"))
    metadata["Orientation"] = str(tags.get("Image Orientation", "Unknown"))

    # Extract timestamp and ensure correct format
    datetime_str = str(tags.get("EXIF DateTimeOriginal", ""))
    if datetime_str and datetime_str != "Unknown":
        try:
            dt_obj = datetime.strptime(datetime_str, "%Y:%m:%d %H:%M:%S")
            # Convert to ISO8601
            metadata["DateTime"] = dt_obj.isoformat() + "Z"
        except ValueError:
            metadata["DateTime"] = None
    else:
        metadata["DateTime"] = None

    # Convert GPS coordinates to decimal format
    lat_values = tags.get("GPS GPSLatitude", None)
    lon_values = tags.get("GPS GPSLongitude", None)
    if lat_values and lon_values:
        metadata["GPS Latitude"] = convert_gps_to_decimal(lat_values.values)
        metadata["GPS Longitude"] = convert_gps_to_decimal(lon_values.values)
    else:
        metadata["GPS Latitude"] = None
        metadata["GPS Longitude"] = None

    return metadata


# ------------------------------------------------------------------------------
# 4. Chat Parsing (Uses parse_message + date from filename)
# ------------------------------------------------------------------------------
def parse_chat_file(file_path):
    """
    Calls `parse_message` as a subprocess, decides whether `--html` or `--json`,
    and extracts a date (YYYY-MM-DD) from the filename (if any).
    """
    _, ext = os.path.splitext(file_path.lower())
    fname = os.path.basename(file_path)

    # If .json, pass `--json`; otherwise `--html`.
    if ext == ".json":
        parse_flag = "--json"
    else:
        parse_flag = "--html"

    # Extract date from filename
    date_str = extract_date_from_filename(fname)

    cmd = ["./parse_message", parse_flag, date_str]
    try:
        with open(file_path, "rb") as f_in:
            result = subprocess.run(
                cmd, stdin=f_in, capture_output=True, text=True, check=True
            )
        return json.loads(result.stdout)
    except subprocess.CalledProcessError as e:
        print(f"[ERROR] parse_message failed on {file_path}: {e}", file=sys.stderr)
    except json.JSONDecodeError as e:
        print(
            f"[ERROR] Invalid JSON from parse_message on {file_path}: {e}",
            file=sys.stderr,
        )
    return []


# ------------------------------------------------------------------------------
# 5. MyAnimeList Parsing (Placeholder)
# ------------------------------------------------------------------------------
def parse_myanimelist(xml_file):
    """
    Example placeholder: parse 'animelist.xml' and produce minimal events.
    Adjust to your real MyAnimeList XML structure.
    """
    events = []
    if not os.path.isfile(xml_file):
        print(f"[MYANIMELIST] No file at {xml_file}")
        return events

    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()

        # Adjust these tags for your actual MyAnimeList export structure
        anime_list = root.findall("anime")
        for anime in anime_list:
            title = anime.findtext("series_title", "Unknown Title")
            watched_eps = anime.findtext("my_watched_episodes", "0")
            status = anime.findtext("my_status", "Unknown")
            events.append(
                {
                    "author": "unspecified",
                    "timestamp": "1970-01-01T00:00:00Z",
                    "source": "myanimelist",
                    "message": f"Watched {title} (episodes: {watched_eps})",
                    "metadata": {
                        "anime_title": title,
                        "episode_count": watched_eps,
                        "status": status,
                    },
                }
            )
    except ET.ParseError as e:
        print(
            f"[ERROR] Could not parse MyAnimeList XML {xml_file}: {e}", file=sys.stderr
        )

    return events


# ------------------------------------------------------------------------------
# 6. Reddit Parsing (via xform.reddit_parser)
# ------------------------------------------------------------------------------
def parse_reddit_directory(data_dir):
    if not REDDIT_AVAILABLE:
        print("[WARNING] reddit_parser not available; skipping Reddit.")
        return []

    if not os.path.isdir(data_dir):
        print(f"[ERROR] Reddit directory not found: {data_dir}", file=sys.stderr)
        return []

    from xform.reddit_parser import (
        RedditStatisticsParser,
        RedditCommentHeadersParser,
        RedditPostsHeadersParser,
        RedditCommentsParser,
        RedditPostsParser,
    )

    statistics_file = os.path.join(data_dir, "statistics.csv")
    comments_file = os.path.join(data_dir, "comments.csv")
    posts_file = os.path.join(data_dir, "posts.csv")
    comment_headers_file = os.path.join(data_dir, "comment_headers.csv")
    post_headers_file = os.path.join(data_dir, "post_headers.csv")

    # Extract username from stats
    username = RedditStatisticsParser.parse(statistics_file)

    # Load headers
    comment_headers = {}
    post_headers = {}
    if os.path.exists(comment_headers_file):
        comment_headers = RedditCommentHeadersParser.parse(comment_headers_file)
    if os.path.exists(post_headers_file):
        post_headers = RedditPostsHeadersParser.parse(post_headers_file)

    # Parse comments
    comments_data = []
    if os.path.exists(comments_file):
        comments_parser = RedditCommentsParser(username, comment_headers)
        comments_data = comments_parser.parse(comments_file)

    # Parse posts
    posts_data = []
    if os.path.exists(posts_file):
        posts_parser = RedditPostsParser(username, post_headers)
        posts_data = posts_parser.parse(posts_file)

    return comments_data + posts_data


# ------------------------------------------------------------------------------
# 7. YouTube Parsing (Example calls parse_plays --html watch-history.html)
# ------------------------------------------------------------------------------
def parse_youtube(raw_youtube_dir):
    events = []
    watch_file = os.path.join(raw_youtube_dir, "watch-history.html")
    if not os.path.isfile(watch_file):
        print(f"[YOUTUBE] No watch-history.html found in {raw_youtube_dir}")
        return events

    try:
        with open(watch_file, "rb") as f_in:
            result = subprocess.run(
                ["./parse_plays", "--html"],
                stdin=f_in,
                capture_output=True,
                text=True,
                check=True,
            )
        events = json.loads(result.stdout)
        print(f"[YOUTUBE] Parsed {len(events)} events from {watch_file}")
    except FileNotFoundError:
        print("[ERROR] parse_plays not found or not executable. Skipping YouTube.")
    except subprocess.CalledProcessError as e:
        print(f"[ERROR] parse_plays failed on {watch_file}: {e}", file=sys.stderr)
    except json.JSONDecodeError as e:
        print(
            f"[ERROR] Invalid JSON from parse_plays for YouTube: {e}", file=sys.stderr
        )

    return events


# ------------------------------------------------------------------------------
# 8. Main Orchestrator
# ------------------------------------------------------------------------------
def main():
    """
    This script processes:
      - Images in `userdata/raw/images`
      - Chat logs in `userdata/raw/chat` (extracting date from filename)
      - MyAnimeList in `userdata/raw/myanimelist`
      - Reddit in `userdata/raw/reddit`
      - YouTube in `userdata/raw/youtube`
    and writes outputs to `userdata/transformed/<service>`.
    """

    # Ensure output dirs exist
    for d in [X_IMAGES, X_CHAT, X_MYANIMELIST, X_REDDIT, X_YOUTUBE]:
        os.makedirs(d, exist_ok=True)

    # ---------------- IMAGES
    all_images_meta = []
    for root, _, files in os.walk(RAW_IMAGES):
        for fname in files:
            if fname.lower().endswith((".jpg", ".jpeg")):
                full_path = os.path.join(root, fname)
                exif_data = extract_exif(full_path)
                if exif_data:
                    all_images_meta.append(exif_data)
                    print(f"[IMG] Processed: {full_path}")
    images_outfile = os.path.join(X_IMAGES, "images_exif.json")
    with open(images_outfile, "w", encoding="utf-8") as f_img:
        json.dump(all_images_meta, f_img, indent=4)
    print(f"[IMG] Wrote {len(all_images_meta)} records to {images_outfile}")

    # ---------------- CHAT
    chat_count = 0
    for root, _, files in os.walk(RAW_CHAT):
        for fname in files:
            if fname.lower().endswith((".htm", ".html", ".txt", ".json")):
                full_path = os.path.join(root, fname)
                events = parse_chat_file(full_path)
                if events:
                    base, _ = os.path.splitext(fname)
                    out_json = os.path.join(X_CHAT, base + ".json")
                    with open(out_json, "w", encoding="utf-8") as fout:
                        json.dump(events, fout, indent=4)
                    chat_count += 1
                    print(f"[CHAT] Processed {full_path} -> {out_json}")
    print(f"[CHAT] Processed {chat_count} chat file(s). Output in {X_CHAT}")

    # ---------------- MYANIMELIST
    myanimelist_xml = os.path.join(RAW_MYANIMELIST, "animelist.xml")
    myanimelist_events = parse_myanimelist(myanimelist_xml)
    myanimelist_out = os.path.join(X_MYANIMELIST, "myanimelist.json")
    with open(myanimelist_out, "w", encoding="utf-8") as f_myan:
        json.dump(myanimelist_events, f_myan, indent=4)
    print(
        f"[MYANIMELIST] Parsed {len(myanimelist_events)} events. Wrote {myanimelist_out}"
    )

    # ---------------- REDDIT
    reddit_events = parse_reddit_directory(RAW_REDDIT)
    reddit_out = os.path.join(X_REDDIT, "reddit.json")
    with open(reddit_out, "w", encoding="utf-8") as f_red:
        json.dump(reddit_events, f_red, indent=4)
    print(f"[REDDIT] Parsed {len(reddit_events)} events. Wrote {reddit_out}")

    # ---------------- YOUTUBE
    youtube_events = parse_youtube(RAW_YOUTUBE)
    youtube_out = os.path.join(X_YOUTUBE, "youtube.json")
    with open(youtube_out, "w", encoding="utf-8") as f_yt:
        json.dump(youtube_events, f_yt, indent=4)
    print(f"[YOUTUBE] Parsed {len(youtube_events)} events. Wrote {youtube_out}")

    print("\nAll data processed successfully.")


if __name__ == "__main__":
    main()
