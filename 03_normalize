#!/usr/bin/env python3
import os
import json
import re
import subprocess
import logging
from collections import defaultdict
from datetime import datetime, timedelta

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)


class DataIngest:
    """Handles the ingestion and transformation of Niantic and Lyft JSON data."""

    SOURCE_PATHS = {
        "pokemongo": "userdata/transformed/pokemongo",
        "lyft": "userdata/transformed/lyft",
        "waze": "userdata/transformed/waze",
        "netflix": "userdata/transformed/netflix",
        "spotify": "userdata/transformed/spotify/Spotify Account Data",
        "images": "userdata/transformed/images",
        "chat": "userdata/transformed/chat",
        "github": "userdata/transformed/github",
    }
    OUTPUT_PATHS = {
        "pokemongo": "userdata/purposed/pokemongo",
        "lyft": "userdata/purposed/lyft",
        "waze": "userdata/purposed/waze",
        "netflix": "userdata/purposed/netflix",
        "spotify": "userdata/purposed/spotify",
        "images": "userdata/purposed/images",
        "chat": "userdata/purposed/chat",
        "github": "userdata/purposed/github",
    }
    PIPE_SCRIPT = "pipes/rewrite_pipe_of_sorts.py"

    TIMESTAMP_FIELDS = {
        "Date and time of logging (UTC)",
        "searchTime",
        "Last Login",
    }
    TIMESTAMP_SUFFIXES = {"ts", "timestamp", "date", "time", "_at"}
    TIMESTAMP_FORMATS = [
        "%m/%d/%Y %H:%M:%S UTC",  # Matches: 02/17/2025 22:30:48 UTC
        "%Y-%m-%dT%H:%M:%S.%fZ",  # Matches 2024-11-06T05:03:24.791Z
        "%Y-%m-%dT%H:%M:%S.%f",  # Matches 2021-02-04T22:01:38.738000
        "%Y-%m-%dT%H:%M:%SZ",  # Matches 2021-11-15T15:21:48Z
        "%Y-%m-%d %H:%M:%S.%f UTC",
        "%Y-%m-%d %H:%M:%S UTC",
        "%Y-%m-%d %H:%M:%S GMT",  # Matches: 2021-12-24 21:13:56 GMT
        "%Y-%m-%dT%H:%M:%S.%f",  # Matches: 2021-06-03T00:05:00.198
        "%Y-%m-%d %H:%M:%S.%f%z",  # Matches: 2011-02-07 10:16:18.000+0000
        "%Y-%m-%dT%H:%M:%S.%fZ",  # Matches: 2018-09-02T02:15:36Z
        "%Y-%m-%dT%H:%M:%S%z",  # Matches: 2011-09-29T07:30:11+0000
        "%Y-%m-%dT%H:%M:%S",  # Matches: 2021-02-01T03:47:54
        "%Y-%m-%d %H:%M",  # Matches: 2021-02-05 11:19
        "%Y%m%d",
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%d",
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%d %H:%M:%S.%f",
    ]

    LAT_LONG_FIELDS = {"latitude", "longitude", "lat", "lng"}
    LAT_LONG_SUFFIXES = {
        "_lat",
        "_lng",
        "_latitude",
        "_longitude",
        "location reported by game",
    }

    def __init__(self):
        """Initialize output directories and aggregated data stores."""
        for path in self.OUTPUT_PATHS.values():
            os.makedirs(path, exist_ok=True)
        self.aggregated_data = {
            "pokemongo": defaultdict(list),
            "lyft": defaultdict(list),
            "waze": defaultdict(list),
            "spotify": defaultdict(list),
            "netflix": defaultdict(list),
            "images": defaultdict(list),
            "chat": defaultdict(list),
            "github": defaultdict(list),
        }

    def convert_lat_long_fields(self, data):
        """Identify latitude/longitude fields and convert them to valid floats while ensuring data integrity."""

        if not data or not isinstance(data, list):
            # logging.warning("Data is empty or not a list.")
            return []

        # Identify lat/long fields based on the first record
        first_record = data[0]
        lat_long_keys = {
            key
            for key in first_record.keys()
            if key.lower() in self.LAT_LONG_FIELDS
            or any(key.lower().endswith(suffix) for suffix in self.LAT_LONG_SUFFIXES)
        }

        if not lat_long_keys:
            # logging.info("No latitude/longitude fields detected. Skipping conversion.")
            return data  # No lat/long fields found, return data as is

        logging.info(f"Identified latitude/longitude fields: {lat_long_keys}")

        valid_data = []  # Store valid records
        fields_changed = 0

        for record in data:
            for key in lat_long_keys:
                value = record.get(key)

                if value == "":
                    record[key] = None
                    fields_changed += 1
                    continue

                # Try converting to float
                try:
                    record[key] = float(value)  # Replace value with valid float
                except ValueError:
                    logging.warning(
                        f"Skipping record due to non-numeric value in field '{key}': {value}."
                    )
                    break  # Prevent saving it

            valid_data.append(record)

        if len(valid_data) != len(data):
            logging.info(f"total kept: {len(valid_data)} / total checked: {len(data)}")
        else:
            logging.info(
                f"{fields_changed} values modified; total records {len(valid_data)}"
            )
        return valid_data

    def convert_iso_timestamps(self, data):
        """Convert all timestamp-like fields to ISO 8601 format, tracking the number of modifications."""

        if not data or not isinstance(data, list):
            # logging.warning("Data is empty or not a list.")
            return []

        # Identify timestamp fields based on the first record
        first_record = data[0]
        timestamp_keys = {
            key
            for key in first_record.keys()
            if key in self.TIMESTAMP_FIELDS
            or any(key.lower().endswith(suffix) for suffix in self.TIMESTAMP_SUFFIXES)
        }

        if not timestamp_keys:
            # logging.info("No timestamp fields detected. Skipping conversion.")
            return data  # No timestamp fields found, return data as is

        logging.info(f"Identified timestamp fields: {timestamp_keys}")

        valid_data = []  # Store valid records
        modifications_count = {}  # Tracks how many values were changed per field

        for key in timestamp_keys:
            modifications_count[key] = 0  # Initialize each field count to 0

        for record in data:
            for key in timestamp_keys:
                value = record.get(key)

                # Convert empty string to None
                if value == "":
                    record[key] = None
                    modifications_count[key] += 1
                    # logging.info(f"Converted empty string to None in field '{key}'.")
                    continue  # Move to the next field

                # Ensure value is a valid string
                if not isinstance(value, str):
                    logging.warning(
                        f"Skipping record due to non-string value in field '{key}': '{value}'"
                    )
                    continue  # Move to the next field

                if "[UTC]" in value:
                    # z means UTC so strip it out
                    value = value.replace("[UTC]", "")

                # Attempt conversion using multiple formats
                parsed_timestamp = None
                for fmt in self.TIMESTAMP_FORMATS:
                    try:
                        parsed_timestamp = datetime.strptime(
                            value.strip(), fmt
                        ).isoformat()
                        break  # Stop trying formats once successful
                    except ValueError:
                        continue

                if parsed_timestamp:
                    record[
                        key
                    ] = parsed_timestamp  # Replace with ISO 8601 formatted timestamp
                    # modifications_count[key] += 1  # proper parsing not considered as modifying
                else:
                    logging.warning(
                        f"Could not parse timestamp format in field '{key}': {value}"
                    )
                    logging.warning(record)
                    record[key] = None  # Assign None if parsing fails
                    modifications_count[key] += 1

            valid_data.append(record)

        # Logging summary of modifications
        total_modifications = sum(modifications_count.values())
        if total_modifications > 0:
            logging.info(
                f"{total_modifications} total timestamp values interpreted as None across {len(valid_data)} records."
            )
            logging.info(f"Modification details: {modifications_count}")
        else:
            logging.info("No timestamp values were modified.")

        return valid_data

    def transform(self, dataset, json_path):
        """Determine the correct transformation for a given file."""
        filename = os.path.basename(json_path)
        base_filename = re.sub(r"\d+", "", filename).strip()

        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except json.JSONDecodeError:
            logging.warning(f"Skipping {filename}: Invalid JSON format")
            return

        data = self.convert_lat_long_fields(data)
        data = self.convert_iso_timestamps(data)
        self.aggregated_data[dataset][base_filename].extend(data)

    def process_files(self, dataset):
        """Process all JSON files in the source directory."""
        source_path = self.SOURCE_PATHS[dataset]
        output_path = self.OUTPUT_PATHS[dataset]

        for filename in os.listdir(source_path):
            input_filepath = os.path.join(source_path, filename)

            if not filename.endswith(".json"):
                continue  # Skip non-JSON files

            # Special handling for "waze/general_info.json"
            if dataset == "waze" and filename == "general_info.json":
                logging.info(f"Handling edge-case file: {filename}")
                output_filepath = os.path.join(output_path, filename)
                filename = os.path.basename(input_filepath)
                base_filename = re.sub(r"\d+", "", filename).strip()

                try:
                    # Read the original JSON content
                    with open(input_filepath, "r", encoding="utf-8") as f:
                        original_data = json.load(f)

                    # Process through rewrite_transpose.py
                    result = subprocess.run(
                        ["./pipes/rewrite_transpose.py"],
                        input=json.dumps(original_data, indent=4),
                        text=True,
                        capture_output=True,
                        check=True,
                    )

                    # Load the modified JSON
                    processed_data = json.loads(result.stdout)

                    # Apply transformations
                    processed_data = self.convert_lat_long_fields(processed_data)
                    processed_data = self.convert_iso_timestamps(processed_data)

                    # Write transformed data
                    with open(output_filepath, "w", encoding="utf-8") as f:
                        json.dump(processed_data, f, indent=4)

                    logging.info(f"Post-processed: {filename} → {output_filepath}")
                except subprocess.CalledProcessError as e:
                    logging.error(
                        f"Error processing {filename} with rewrite_transpose.py: {e}"
                    )
            elif dataset == "waze" and filename == "search_history.json":
                logging.info(f"Handling edge-case file: {filename}")
                output_filepath = os.path.join(output_path, filename)
                filename = os.path.basename(input_filepath)
                base_filename = re.sub(r"\d+", "", filename).strip()

                try:
                    # Read the original JSON content
                    with open(input_filepath, "r", encoding="utf-8") as f:
                        original_data = json.load(f)

                    # Process through rewrite_transpose.py
                    field_names = [
                        "timestamp",
                        "search_terms",
                        "latitude",
                        "longitude",
                    ]  # Example field names

                    result = subprocess.run(
                        ["./pipes/rewrite_fieldnames.py"]
                        + field_names,  # Pass field names as arguments
                        input=json.dumps(original_data, indent=4),
                        text=True,
                        capture_output=True,
                        check=True,
                    )

                    # Load the modified JSON
                    processed_data = json.loads(result.stdout)

                    # Apply transformations
                    processed_data = self.convert_lat_long_fields(processed_data)
                    processed_data = self.convert_iso_timestamps(processed_data)

                except TypeError as e:
                    logging.error(
                        f"Error processing {filename} with rewrite_fieldnames.py: {e}"
                    )
                except subprocess.CalledProcessError as e:
                    logging.error(
                        f"Error processing {filename} with rewrite_fieldnames.py: {e}"
                    )
                else:
                    # Define a local test
                    def is_valid_lat_long(record):
                        """Checks if the record has valid numerical latitude and longitude."""
                        try:
                            lat = record.get("latitude")
                            lon = record.get("longitude")

                            # Ensure latitude & longitude are present and convertible to float
                            if lat is None or lon is None:
                                return False

                            lat = float(lat)
                            lon = float(lon)

                            # Ensure they are within valid geographic bounds
                            return -90 <= lat <= 90 and -180 <= lon <= 180

                        except (ValueError, TypeError):
                            return False  # Non-numeric or invalid data

                    # Write transformed data
                    record_count = len(processed_data)
                    processed_data = [
                        record for record in processed_data if is_valid_lat_long(record)
                    ]
                    logging.warning(
                        f"Dropped {record_count - len(processed_data)} invalid records."
                    )
                    with open(output_filepath, "w", encoding="utf-8") as f:
                        json.dump(processed_data, f, indent=4)

                    logging.info(f"Post-processed: {filename} → {output_filepath}")
            else:
                # Process all other JSON files normally
                self.transform(dataset, input_filepath)

    def write_output(self, dataset):
        """Write all aggregated output files to the destination directory."""
        output_path = self.OUTPUT_PATHS[dataset]

        # Gather all base filenames for this dataset from aggregated data
        base_filenames = list(self.aggregated_data[dataset].keys())

        # Local function to compute dynamic width based on average length of filenames,
        # disregarding the 2 longest files.
        def compute_dynamic_width(filenames):
            if not filenames:
                return 50  # default width if no files are found
            if len(filenames) <= 2:
                return max(len(name) for name in filenames)
            lengths = sorted(len(name) for name in filenames)
            trimmed = lengths[-2]  # base on 2 longest filename
            return trimmed

        dynamic_width = compute_dynamic_width(base_filenames)

        for base_filename, data in self.aggregated_data[dataset].items():
            output_filepath = os.path.join(output_path, base_filename)

            # Write JSON normally
            with open(output_filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=4, ensure_ascii=False)
            logging.info(f"Saved: {base_filename:<{dynamic_width}} → {output_filepath}")

    def run(self):
        """Run the full ingestion process for all datasets."""
        for dataset in [
            "pokemongo",
            "lyft",
            "waze",
            "spotify",
            "netflix",
            "images",
            "chat",
            "github",
        ]:
            self.process_files(dataset)
            self.write_output(dataset)


if __name__ == "__main__":
    DataIngest().run()
