#!/usr/bin/env python3
import os
import json
import re
import subprocess
import logging
from collections import defaultdict
from datetime import datetime, timedelta

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)


class DataIngest:
    """Handles the ingestion and transformation of Niantic and Lyft JSON data."""

    SOURCE_PATHS = {
        "pokemongo": "userdata/transformed/pokemongo",
        "lyft": "userdata/transformed/lyft",
        "waze": "userdata/transformed/waze",
        "netflix": "userdata/transformed/netflix",
        "spotify": "userdata/transformed/spotify/Spotify Account Data",
        "images": "userdata/transformed/images",
        "chat": "userdata/transformed/chat",
        "github": "userdata/transformed/github",
        "geojson": "userdata/transformed/geojson",
    }
    OUTPUT_PATHS = {
        "pokemongo": "userdata/purposed/pokemongo",
        "lyft": "userdata/purposed/lyft",
        "waze": "userdata/purposed/waze",
        "netflix": "userdata/purposed/netflix",
        "spotify": "userdata/purposed/spotify",
        "images": "userdata/purposed/images",
        "chat": "userdata/purposed/chat",
        "github": "userdata/purposed/github",
        "geojson": "userdata/purposed/geojson",
    }
    PIPE_SCRIPT = "pipes/rewrite_pipe_of_sorts.py"

    TIMESTAMP_FIELDS = {
        "Date and time of logging (UTC)",
        "searchTime",
        "Last Login",
    }
    TIMESTAMP_SUFFIXES = {"ts", "timestamp", "date", "time", "_at"}
    TIMESTAMP_FORMATS = [
        "%Y-%m-%d %H:%M:%S.%f UTC",  # Example: 2021-12-24 21:13:56.123456 UTC
        "%Y-%m-%d %H:%M:%S UTC",  # Example: 2021-12-24 21:13:56 UTC
        "%m/%d/%Y %H:%M:%S UTC",  # Example: 02/17/2025 22:30:48 UTC
        "%Y-%m-%d %H:%M:%S GMT",  # Example: 2021-12-24 21:13:56 GMT
        "%Y-%m-%dT%H:%M:%S.%fZ",  # Example: 2024-11-06T05:03:24.791Z
        "%Y-%m-%dT%H:%M:%SZ",  # Example: 2021-11-15T15:21:48Z
        "%Y-%m-%dT%H:%M:%S.%f",  # Example: 2021-06-03T00:05:00.198
        "%Y-%m-%d %H:%M:%S.%f%z",  # Example: 2011-02-07 10:16:18.000+0000
        "%Y-%m-%dT%H:%M:%S.%f%z",  # Example: 2021-10-06T19:12:03.000-07:00
        "%Y-%m-%dT%H:%M:%S%z",  # Example: 2011-09-29T07:30:11+0000
        "%Y-%m-%dT%H:%M:%S",  # Example: 2021-02-01T03:47:54
        "%Y:%m:%d %H:%M:%S",  # Example: 2015:04:22 18:46:59 (EXIF format)
        "%Y-%m-%d %H:%M",  # Example: 2021-02-05 11:19
        "%Y%m%d",  # Example: 20210310
        "%Y-%m-%d %H:%M:%S",  # Example: 2021-02-01 12:30:45
        "%Y-%m-%d",  # Example: 2021-02-01
        "%Y-%m-%d %H:%M:%S.%f",  # Example: 2021-02-01 12:30:45.123456
    ]

    LAT_LONG_FIELDS = {"latitude", "longitude", "lat", "lng"}
    LAT_LONG_SUFFIXES = {
        "_lat",
        "_lng",
        "_latitude",
        "_longitude",
        "location reported by game",
    }

    def __init__(self):
        """Initialize output directories and aggregated data stores."""
        for path in self.OUTPUT_PATHS.values():
            os.makedirs(path, exist_ok=True)
        self.aggregated_data = {
            "pokemongo": defaultdict(list),
            "lyft": defaultdict(list),
            "waze": defaultdict(list),
            "spotify": defaultdict(list),
            "netflix": defaultdict(list),
            "images": defaultdict(list),
            "chat": defaultdict(list),
            "github": defaultdict(list),
            "geojson": [],
        }

    def convert_lat_long_fields(self, data):
        """Identify latitude/longitude fields and convert them to valid floats while ensuring data integrity."""

        if not data or not isinstance(data, list):
            # logging.warning("Data is empty or not a list.")
            return []

        # Identify lat/long fields based on the first record
        first_record = data[0]
        lat_long_keys = {
            key
            for key in first_record.keys()
            if key.lower() in self.LAT_LONG_FIELDS
            or any(key.lower().endswith(suffix) for suffix in self.LAT_LONG_SUFFIXES)
        }

        if not lat_long_keys:
            # logging.info("No latitude/longitude fields detected. Skipping conversion.")
            return data  # No lat/long fields found, return data as is

        logging.info(f"Identified latitude/longitude fields: {lat_long_keys}")

        valid_data = []  # Store valid records
        fields_changed = 0

        for record in data:
            for key in lat_long_keys:
                value = record.get(key)

                if value == "":
                    record[key] = None
                    fields_changed += 1
                    continue

                # Try converting to float
                try:
                    record[key] = float(value)  # Replace value with valid float
                except ValueError:
                    logging.warning(
                        f"Skipping record due to non-numeric value in field '{key}': {value}."
                    )
                    break  # Prevent saving it

            valid_data.append(record)

        if len(valid_data) != len(data):
            logging.info(f"total kept: {len(valid_data)} / total checked: {len(data)}")
        else:
            logging.info(
                f"{fields_changed} values modified; total records {len(valid_data)}"
            )
        return valid_data

    def convert_iso_timestamps(self, doc: dict) -> dict:
        """
        Recursively traverse 'data' (which can be a dict, list, or otherwise),
        and for any key that appears to be a timestamp field, convert it to ISO8601.
        If the field cannot be parsed with the known formats, set it to None.
        """

        def parse_timestamp(value_str):
            """Try to parse a string using the known TIMESTAMP_FORMATS, returning ISO8601 or None."""
            if not isinstance(value_str, str) or not value_str.strip():
                return None  # Not a valid non-empty string

            # Remove "[UTC]" if present and strip whitespace
            cleaned_str = value_str.replace("[UTC]", "").strip()

            # Attempt parsing with each format
            for fmt in self.TIMESTAMP_FORMATS:
                try:
                    return datetime.strptime(cleaned_str, fmt).isoformat()
                except ValueError:
                    pass

            logging.warning(f"Could not parse timestamp: '{value_str}'")
            return None

        def recurse(obj):
            """Walk through dicts/lists at all nesting levels, converting any recognized timestamp fields."""
            if isinstance(obj, dict):
                new_dict = {}
                for k, v in obj.items():
                    # Check if this key is recognized as a timestamp-like field
                    if k in self.TIMESTAMP_FIELDS or any(
                        k.lower().endswith(suffix) for suffix in self.TIMESTAMP_SUFFIXES
                    ):
                        new_dict[k] = parse_timestamp(v)
                    else:
                        # Keep recursing deeper for non-timestamp fields
                        new_dict[k] = recurse(v)
                return new_dict

            elif isinstance(obj, list):
                # Recurse into each item in the list
                return [recurse(item) for item in obj]

            else:
                # If it's a scalar (str, int, float, etc.) that's not recognized
                # as a timestamp field, return as is
                return obj

        # Begin recursion from the top-level doc
        return recurse(doc)

    def transform(self, dataset, json_path):
        """Determine the correct transformation for a given file."""
        filename = os.path.basename(json_path)

        if dataset == "geojson":
            base_filename = "geojson"
            try:
                with open(json_path, "r", encoding="utf-8") as f:
                    geojson_data = json.load(f)
            except json.JSONDecodeError:
                logging.warning(f"Skipping {filename}: Invalid JSON format")
                return

            if (
                geojson_data.get("type") != "FeatureCollection"
                or "features" not in geojson_data
            ):
                logging.warning(f"Skipping {filename}: Not a valid FeatureCollection")
                return

            # Flatten GeoJSON: Extract each feature's properties & geometry
            for feature in geojson_data["features"]:
                if "properties" in feature and "geometry" in feature:
                    ts = feature["properties"].pop("time")
                    flattened_record = {
                        **feature["properties"],
                        "timestamp": ts,
                        "latitude": feature["geometry"]["coordinates"][1],
                        "longitude": feature["geometry"]["coordinates"][0],
                    }
                    self.aggregated_data[dataset].append(flattened_record)

            logging.info(
                f"Flattened {filename} into {len(geojson_data['features'])} records"
            )
        else:
            # 1. Remove the '.json' extension BEFORE storing in dictionary
            if filename.endswith(".json"):
                base_filename = filename[:-5]  # Strip exactly ".json"
            else:
                base_filename = filename

            # 2. Remove numbers at the end, whether they have an underscore or not
            base_filename = re.sub(
                r"(_?\d+)$", "", base_filename
            )  # Removes _00001, _42, 123, etc.

            # 3. Remove trailing underscores (if any remain after number removal)
            base_filename = base_filename.rstrip("_")

            try:
                with open(json_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
            except json.JSONDecodeError:
                logging.warning(f"Skipping {filename}: Invalid JSON format")
                return

            data = self.convert_lat_long_fields(data)
            data = self.convert_iso_timestamps(data)

            # **Store in dictionary WITHOUT .json, without numbers, and fully cleaned**
            cleaned_key = base_filename.rstrip("_")
            self.aggregated_data[dataset][cleaned_key].extend(data)

    def process_files(self, dataset):
        """Process all JSON files in the source directory."""
        source_path = self.SOURCE_PATHS[dataset]
        output_path = self.OUTPUT_PATHS[dataset]

        for filename in os.listdir(source_path):
            input_filepath = os.path.join(source_path, filename)

            if not filename.endswith(".json"):
                continue  # Skip non-JSON files

            # Special handling for "waze/general_info.json"
            if dataset == "waze" and filename == "general_info.json":
                logging.info(f"Handling edge-case file: {filename}")
                output_filepath = os.path.join(output_path, filename)
                filename = os.path.basename(input_filepath)
                base_filename = re.sub(r"\d+", "", filename).strip()

                try:
                    # Read the original JSON content
                    with open(input_filepath, "r", encoding="utf-8") as f:
                        original_data = json.load(f)

                    # Process through rewrite_transpose.py
                    result = subprocess.run(
                        ["./pipes/rewrite_transpose.py"],
                        input=json.dumps(original_data, indent=4),
                        text=True,
                        capture_output=True,
                        check=True,
                    )

                    # Load the modified JSON
                    processed_data = json.loads(result.stdout)

                    # Apply transformations
                    processed_data = self.convert_lat_long_fields(processed_data)
                    processed_data = self.convert_iso_timestamps(processed_data)

                    # Write transformed data
                    with open(output_filepath, "w", encoding="utf-8") as f:
                        json.dump(processed_data, f, indent=4)

                    logging.info(f"Post-processed: {filename} → {output_filepath}")
                except subprocess.CalledProcessError as e:
                    logging.error(
                        f"Error processing {filename} with rewrite_transpose.py: {e}"
                    )
            elif dataset == "waze" and filename == "search_history.json":
                logging.info(f"Handling edge-case file: {filename}")
                output_filepath = os.path.join(output_path, filename)
                filename = os.path.basename(input_filepath)
                base_filename = re.sub(r"\d+", "", filename).strip()

                try:
                    # Read the original JSON content
                    with open(input_filepath, "r", encoding="utf-8") as f:
                        original_data = json.load(f)

                    # Process through rewrite_transpose.py
                    field_names = [
                        "timestamp",
                        "search_terms",
                        "latitude",
                        "longitude",
                    ]  # Example field names

                    result = subprocess.run(
                        ["./pipes/rewrite_fieldnames.py"]
                        + field_names,  # Pass field names as arguments
                        input=json.dumps(original_data, indent=4),
                        text=True,
                        capture_output=True,
                        check=True,
                    )

                    # Load the modified JSON
                    processed_data = json.loads(result.stdout)

                    # Apply transformations
                    processed_data = self.convert_lat_long_fields(processed_data)
                    processed_data = self.convert_iso_timestamps(processed_data)

                except TypeError as e:
                    logging.error(
                        f"Error processing {filename} with rewrite_fieldnames.py: {e}"
                    )
                except subprocess.CalledProcessError as e:
                    logging.error(
                        f"Error processing {filename} with rewrite_fieldnames.py: {e}"
                    )
                else:
                    # Define a local test
                    def is_valid_lat_long(record):
                        """Checks if the record has valid numerical latitude and longitude."""
                        try:
                            lat = record.get("latitude")
                            lon = record.get("longitude")

                            # Ensure latitude & longitude are present and convertible to float
                            if lat is None or lon is None:
                                return False

                            lat = float(lat)
                            lon = float(lon)

                            # Ensure they are within valid geographic bounds
                            return -90 <= lat <= 90 and -180 <= lon <= 180

                        except (ValueError, TypeError):
                            return False  # Non-numeric or invalid data

                    # Write transformed data
                    record_count = len(processed_data)
                    processed_data = [
                        record for record in processed_data if is_valid_lat_long(record)
                    ]
                    logging.warning(
                        f"Dropped {record_count - len(processed_data)} invalid records."
                    )
                    with open(output_filepath, "w", encoding="utf-8") as f:
                        json.dump(processed_data, f, indent=4)

                    logging.info(f"Post-processed: {filename} → {output_filepath}")
            else:
                # Process all other JSON files normally
                self.transform(dataset, input_filepath)

    def write_output(self, dataset):
        """Write all aggregated output files to the destination directory."""
        output_path = self.OUTPUT_PATHS[dataset]

        if dataset == "geojson":
            output_filename = f"geojson.json"
            output_filepath = os.path.join(output_path, output_filename)

            data = self.aggregated_data[dataset]
            # Write the cleaned and aggregated JSON data
            with open(output_filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=4, ensure_ascii=False)

            logging.info(f"Saved: {output_filename} → {output_filepath}")
        else:
            for base_filename, data in self.aggregated_data[dataset].items():
                # Ensure the filename ends with .json and strip any remaining underscores
                output_filename = f"{base_filename.rstrip('_')}.json"
                output_filepath = os.path.join(output_path, output_filename)

                # Write the cleaned and aggregated JSON data
                with open(output_filepath, "w", encoding="utf-8") as f:
                    json.dump(data, f, indent=4, ensure_ascii=False)

                logging.info(f"Saved: {output_filename} → {output_filepath}")

    def run(self):
        """Run the full ingestion process for all datasets."""
        for dataset in [
            "pokemongo",
            "lyft",
            "waze",
            "spotify",
            "netflix",
            # "images",
            # "chat",
            "github",
            "geojson",
        ]:
            self.process_files(dataset)
            self.write_output(dataset)


if __name__ == "__main__":
    DataIngest().run()
