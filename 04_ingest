#!/usr/bin/env python3

import os
import re
import json
import logging
import pymongo
from datetime import datetime
from dateutil.parser import parse as parse_datetime

"""
04_ingest.py

A script for ingesting JSON files into MongoDB. 
Ensures all datetime fields are converted properly before insertion, 
handles duplicates, and sanitizes field names.

Requirements:
 - Python 3
 - PyMongo
 - python-dateutil
"""

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# MongoDB Connection Environment Variables
MONGO_USER = os.getenv("MONGO_USER", "admin")
MONGO_PASSWORD = os.getenv("MONGO_PASSWORD", "password")
MONGO_HOST = "localhost"
MONGO_PORT = "27017"
MONGO_URI = f"mongodb://{MONGO_USER}:{MONGO_PASSWORD}@{MONGO_HOST}:{MONGO_PORT}/admin"

# Directory structure and data sources
BASE_DATA_DIR = "userdata/purposed"
DATA_SOURCES_FILE = "data_sources.json"

try:
    with open(DATA_SOURCES_FILE, "r") as f:
        data_sources = json.load(f)
except Exception as e:
    logging.error(f"Error loading {DATA_SOURCES_FILE}: {e}")
    data_sources = {}


def get_mongo_client():
    """
    Establish and return a MongoDB client using MONGO_URI.
    """
    return pymongo.MongoClient(MONGO_URI)


def sanitize_field_name(field_name: str) -> str:
    """
    Replace non-alphanumeric characters in field names with underscores.
    """
    return re.sub(r"[^a-zA-Z0-9_]", "_", field_name)


def is_potential_datetime(value: str) -> bool:
    """
    A quick heuristic to decide whether a string might be a datetime.
    Adjust the logic/patterns to fit your data.
    """
    import re

    # Optional first check: must be at least 10 chars ("YYYY-MM-DD" is 10).
    if len(value) < 10:
        return False

    # This regex checks for a pattern like YYYY-MM-DD, possibly with time.
    # Feel free to make this more/less strict as needed.
    date_pattern = re.compile(r"^\d{4}-\d{2}-\d{2}")

    if date_pattern.match(value):
        return True
    else:
        return False


def convert_iso_timestamps(doc: dict) -> dict:
    """
    Attempt to parse only those string values that match our 'potential datetime' check.
    If parse is successful, store it as a datetime; otherwise, leave it as-is.
    """
    from dateutil.parser import parse as parse_datetime

    for key, val in list(doc.items()):
        if isinstance(val, str) and is_potential_datetime(val):
            try:
                doc[key] = parse_datetime(val)
            except (ValueError, TypeError):
                # If it fails to parse, just leave the value as a string
                logging.debug(f"Skipping invalid datetime: '{val}' for field '{key}'")
    return doc


def check_document_exists(collection, doc: dict) -> bool:
    """
    Check if an identical document already exists in the specified collection.

    Args:
        collection (Collection): A MongoDB collection object.
        doc (dict): The document to check.

    Returns:
        bool: True if the document exists, False otherwise.
    """
    return collection.count_documents(doc, limit=1) > 0


def ingest_data():
    """
    Ingest JSON files into their respective MongoDB databases/collections.

    The logic is driven by a 'data_sources.json' mapping file.
    Each key in data_sources corresponds to a database name,
    and the values map to directories containing .json files to be ingested.
    """
    client = get_mongo_client()

    if not os.path.exists(BASE_DATA_DIR):
        logging.warning(f"Base data directory '{BASE_DATA_DIR}' does not exist.")
        return

    # Iterate through the databases specified in data_sources.json
    for db_name, _ in data_sources.items():
        db_path = os.path.join(BASE_DATA_DIR, db_name)
        if not os.path.exists(db_path):
            logging.warning(f"Skipping missing directory: {db_path}")
            continue

        logging.info(f"Processing database: {db_name}")
        db = client[db_name]

        # Loop through .json files in the current database folder
        for filename in os.listdir(db_path):
            if filename.endswith(".json"):
                collection_name = os.path.splitext(filename)[0]
                collection = db[collection_name]
                file_path = os.path.join(db_path, filename)

                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        data = json.load(f)

                        # If the file is empty, skip it
                        if not data:
                            logging.info(f"Skipping empty .json file: {filename}")
                            continue

                        # Enforce sanitization and timestamp conversion
                        if isinstance(data, list):
                            data = [
                                convert_iso_timestamps(
                                    {sanitize_field_name(k): v for k, v in doc.items()}
                                )
                                for doc in data
                            ]
                        else:
                            # If the data is a single dict, convert it to a list of one
                            single_doc = {
                                sanitize_field_name(k): v for k, v in data.items()
                            }
                            single_doc = convert_iso_timestamps(single_doc)
                            data = [single_doc]

                        # Insert documents, avoiding duplicates
                        if collection.estimated_document_count() == 0:
                            collection.insert_many(data)
                            logging.info(
                                f"Inserted {len(data):6d} documents "
                                f"into new collection '{collection_name}'."
                            )
                        else:
                            duplicate_count = 0
                            for doc in data:
                                if not check_document_exists(collection, doc):
                                    collection.insert_one(doc)
                                else:
                                    duplicate_count += 1

                            logging.info(
                                f"Skipped {duplicate_count:6d} duplicate documents "
                                f"in collection '{collection_name}'."
                            )

                except Exception as e:
                    logging.error(f"Error processing {filename} in {db_name}: {e}")

    client.close()


if __name__ == "__main__":
    ingest_data()
