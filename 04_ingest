#!/usr/bin/env python3

import os
import re
import json
import logging
import pymongo
from datetime import datetime
from dateutil.parser import parse as parse_datetime

"""
04_ingest.py

A script for ingesting JSON files into MongoDB. 
Ensures all datetime fields are converted properly before insertion (including nested fields), 
handles duplicates, and sanitizes field names (recursively).

Requirements:
 - Python 3
 - PyMongo
 - python-dateutil
"""

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# MongoDB Connection Environment Variables
MONGO_USER = os.getenv("MONGO_USER", "admin")
MONGO_PASSWORD = os.getenv("MONGO_PASSWORD", "password")
MONGO_HOST = "localhost"
MONGO_PORT = "27017"
MONGO_URI = f"mongodb://{MONGO_USER}:{MONGO_PASSWORD}@{MONGO_HOST}:{MONGO_PORT}/admin"

# Directory structure and data sources
BASE_DATA_DIR = "userdata/purposed"
DATA_SOURCES_FILE = "data_sources.json"

try:
    with open(DATA_SOURCES_FILE, "r") as f:
        data_sources = json.load(f)
except Exception as e:
    logging.error(f"Error loading {DATA_SOURCES_FILE}: {e}")
    data_sources = {}


def get_mongo_client():
    """
    Establish and return a MongoDB client using MONGO_URI.
    """
    return pymongo.MongoClient(MONGO_URI)


def sanitize_field_name(field_name: str) -> str:
    """
    Replace non-alphanumeric characters in field names with underscores.
    """
    return re.sub(r"[^a-zA-Z0-9_]", "_", field_name)


def looks_like_datetime(value: str) -> bool:
    """
    Decide whether a string might be a datetime based on length and basic pattern.
    Adjust logic as needed.
    """
    if not isinstance(value, str):
        return False
    # Simple example check: at least 'YYYY-MM-DD' => 10 chars.
    if len(value) < 10:
        return False
    # Basic pattern: starts with YYYY-MM-DD
    return bool(re.match(r"^\d{4}-\d{2}-\d{2}", value.strip()))


def sanitize_and_parse(obj):
    """
    Recursively traverse `obj`.
    1) Sanitize all dict keys.
    2) Convert any string that looks like a datetime to a Python datetime object.
    Returns a new object with the same structure.
    """
    if isinstance(obj, dict):
        new_dict = {}
        for raw_key, val in obj.items():
            # Sanitize the key
            clean_key = sanitize_field_name(raw_key)

            # Recursively process the value
            new_dict[clean_key] = sanitize_and_parse(val)
        return new_dict

    elif isinstance(obj, list):
        # Process each item in the list
        return [sanitize_and_parse(item) for item in obj]

    else:
        # If it's neither list nor dict, check if it "looks" like a datetime
        if isinstance(obj, str) and looks_like_datetime(obj):
            try:
                return parse_datetime(obj)
            except (ValueError, TypeError):
                # If parse fails, leave it as-is
                logging.debug(f"Skipping invalid datetime: '{obj}'")
        # Return original obj if not a candidate for datetime
        return obj


def check_document_exists(collection, doc: dict) -> bool:
    """
    Check if an identical document already exists in the specified collection.

    Args:
        collection (Collection): A MongoDB collection object.
        doc (dict): The document to check.

    Returns:
        bool: True if the document exists, False otherwise.
    """
    return collection.count_documents(doc, limit=1) > 0


def ingest_data():
    """
    Ingest JSON files into their respective MongoDB databases/collections.

    The logic is driven by a 'data_sources.json' mapping file.
    Each key in data_sources corresponds to a database name,
    and the values map to directories containing .json files to be ingested.
    """
    client = get_mongo_client()

    if not os.path.exists(BASE_DATA_DIR):
        logging.warning(f"Base data directory '{BASE_DATA_DIR}' does not exist.")
        return

    # Iterate through the databases specified in data_sources.json
    for db_name, _ in data_sources.items():
        db_path = os.path.join(BASE_DATA_DIR, db_name)
        if not os.path.exists(db_path):
            logging.warning(f"Skipping missing directory: {db_path}")
            continue

        logging.info(f"Processing database: {db_name}")
        db = client[db_name]

        # Loop through .json files in the current database folder
        for filename in os.listdir(db_path):
            if filename.endswith(".json"):
                collection_name = os.path.splitext(filename)[0]
                collection = db[collection_name]
                file_path = os.path.join(db_path, filename)

                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        raw_data = json.load(f)

                        # If the file is empty, skip it
                        if not raw_data:
                            logging.info(f"Skipping empty .json file: {filename}")
                            continue

                        # Recursively sanitize and parse datetimes
                        if isinstance(raw_data, list):
                            data = [sanitize_and_parse(item) for item in raw_data]
                        else:
                            # If the data is a single dict, convert it to a list of one
                            data = [sanitize_and_parse(raw_data)]

                        # Insert documents, avoiding duplicates
                        if collection.estimated_document_count() == 0:
                            collection.insert_many(data)
                            logging.info(
                                f"Inserted {len(data):6d} documents "
                                f"into new collection '{collection_name}'."
                            )
                        else:
                            duplicate_count = 0
                            for doc in data:
                                if not check_document_exists(collection, doc):
                                    collection.insert_one(doc)
                                else:
                                    duplicate_count += 1

                            logging.info(
                                f"Skipped {duplicate_count:6d} duplicate documents "
                                f"in collection '{collection_name}'."
                            )

                except Exception as e:
                    logging.error(f"Error processing {filename} in {db_name}: {e}")

    client.close()


if __name__ == "__main__":
    ingest_data()
